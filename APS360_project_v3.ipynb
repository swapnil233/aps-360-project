{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44518f61-08e1-48bf-8d76-7622c589fcf4",
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1712101618523,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "44518f61-08e1-48bf-8d76-7622c589fcf4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_UqurXNbMApQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1712101619113,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "_UqurXNbMApQ",
    "outputId": "ecd15718-f8bc-4ed6-f164-8f59768404c9"
   },
   "outputs": [],
   "source": [
    "use_google_colab = False\n",
    "\n",
    "if use_google_colab==True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path_to_project = '/content/drive/MyDrive/School/1. University/Year 4/Semester 2/APS360 - Applied Fundamentals of Deep Learning/Project/'\n",
    "else:\n",
    "    # When using on local machine. Folder 'split_data' must be in the same dir as this file.\n",
    "    path_to_project=\"\"\n",
    "    \n",
    "print(path_to_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea95fd-dc18-4ea9-84b7-e6964f5db9f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1712101619364,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "90ea95fd-dc18-4ea9-84b7-e6964f5db9f0",
    "outputId": "ed1e0092-601b-4f49-a3bd-95b066ef2152"
   },
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a24ba-e5c7-4afc-a6d7-edf999ba8e73",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1712101619364,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "7e1a24ba-e5c7-4afc-a6d7-edf999ba8e73"
   },
   "outputs": [],
   "source": [
    "def count_images_and_percentage_with_partition(root):\n",
    "    partitions = ['train', 'val', 'test']\n",
    "    classes = ['Glass', 'Metal', 'Paper', 'Plastic']\n",
    "    total_images = 0\n",
    "    partition_totals = {}\n",
    "    counts = {}\n",
    "\n",
    "    # Count the images in each partition and class\n",
    "    for partition in partitions:\n",
    "        partition_counts = {}\n",
    "        partition_total = 0\n",
    "        for class_name in classes:\n",
    "            class_path = os.path.join(root, 'split_data', partition, class_name)\n",
    "            image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "            count = len(image_files)\n",
    "            partition_counts[class_name] = count\n",
    "            partition_total += count\n",
    "        counts[partition] = partition_counts\n",
    "        partition_totals[partition] = partition_total\n",
    "        total_images += partition_total\n",
    "\n",
    "    # Print counts and percentages\n",
    "    for partition, partition_counts in counts.items():\n",
    "        print(f\"'{partition}' partition: {partition_totals[partition]} images, {(partition_totals[partition] / total_images) * 100:.2f}% of total\")\n",
    "        for class_name, count in partition_counts.items():\n",
    "            percentage = (count / partition_totals[partition]) * 100\n",
    "            print(f\" - {class_name}: {count} images, {percentage:.2f}% of partition\")\n",
    "        print()\n",
    "\n",
    "    print(f\"Total images across all partitions: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455f394-a19f-44c0-b1d5-0ffcf4736268",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images_and_percentage_with_partition(path_to_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19079532-a68b-480d-b226-f3ca23b2e385",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1712101621271,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "19079532-a68b-480d-b226-f3ca23b2e385"
   },
   "outputs": [],
   "source": [
    "# Modified from Lab 3\n",
    "def get_trash_data_loader(batch_size, img_dim):\n",
    "    \"\"\"\n",
    "    Returns data loaders for the training, validation, and test datasets for the trash classification project.\n",
    "\n",
    "    Args:\n",
    "        batch_size: The number of samples per batch to load.\n",
    "        img_dim: Size to resize the incoming images to. E.g., if using LargeNet128, img_dim=128\n",
    "\n",
    "    Returns:\n",
    "        train_loader: DataLoader for the training set\n",
    "        val_loader: DataLoader for the validation set\n",
    "        test_loader: DataLoader for the test set\n",
    "    \"\"\"\n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_dim, img_dim)),  # Resize to img_dim x img_dim.\n",
    "        transforms.RandomHorizontalFlip(),      # Random horizontal flip 50% chance\n",
    "        transforms.RandomVerticalFlip(),        # Random vertical flip 50% chance\n",
    "        transforms.RandomRotation(45),          # Random rotation by 45 degrees\n",
    "        transforms.ToTensor(),                  # Convert images to tensor\n",
    "    ])\n",
    "\n",
    "    # Paths to the folders. Each class needs to have its own folder. Eg split_data/train/Glass\n",
    "    train_dataset = datasets.ImageFolder(f'{path_to_project}split_data/train', transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(f'{path_to_project}split_data/val', transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(f'{path_to_project}split_data/test', transform=transform)\n",
    "\n",
    "    # Create data loaders\n",
    "    # https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n",
    "    num_workers = 2\n",
    "    pin_memory = True if device == 'cuda' else False\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a2ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few sample images from the dataset \n",
    "train_loader_for_vis, _, _ = get_trash_data_loader(20, 128)\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(train_loader_for_vis))\n",
    "\n",
    "# Make a 5x5 grid from the batch of images\n",
    "out = torchvision.utils.make_grid(inputs, nrow=5)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567cd8a5-e05d-4822-aa85-36a2fcdd3775",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712101621403,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "567cd8a5-e05d-4822-aa85-36a2fcdd3775"
   },
   "outputs": [],
   "source": [
    "class LargeNet256(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNet256, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=10 * 30 * 30, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) #convolution, relu activation, max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x))) #convolution, relu activation, max pooling\n",
    "        x = x.view(-1, 10 * 30 * 30)\n",
    "        x = F.relu(self.fc1(x)) #linear, relu activation\n",
    "        x = self.fc2(x) #linear\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCNN256(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OptimizedCNN256, self).__init__()\n",
    "        \n",
    "        # [input - kernel + 2*padding] / stride + 1\n",
    "\n",
    "        # Conv #1\n",
    "        # [3, 256, 256] -> [5, 252, 252]\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # MaxPooling\n",
    "        # [5, 252, 252] -> [5, 126, 126]\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Conv #2\n",
    "        # [5, 126, 126] -> [10, 122, 122]\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        # After the first layer: (256 - 5 + 1) / 1 = 252, then pooled: 252 / 2 = 126\n",
    "        # After the second layer: (126 - 5 + 1) / 1 = 122, then pooled: 122 / 2 = 61\n",
    "        self.num_flat_features = 10 * 61 * 61\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(self.num_flat_features, 120)\n",
    "\n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.num_flat_features)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCNN128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OptimizedCNN128, self).__init__()\n",
    "        \n",
    "        # Conv #1: [3, 128, 128] -> [5, 124, 124]\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # MaxPooling: [5, 124, 124] -> [5, 62, 62]\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Conv #2: [5, 62, 62] -> [10, 58, 58]\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        # Adjusted for input size of 128x128\n",
    "        self.num_flat_features = 10 * 29 * 29  # 8410\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(self.num_flat_features, 120)\n",
    "\n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.num_flat_features)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf702136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ModifiedAlexNet, self).__init__()\n",
    "        # Using the original AlexNet configuration with modifications\n",
    "        self.features = nn.Sequential(\n",
    "            # Convolutional Layer #1: Input (227x227x3) -> Output (55x55x96)\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True), # Inplace operation to reduce memory usage\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # Input (55x55x96) -> Output (27x27x96)\n",
    "            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),\n",
    "            \n",
    "            # Convolutional Layer #2: Input (55x55x96) -> Output (27x27x256)\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),\n",
    "            \n",
    "            # Convolutional Layer #3: Input (27x27x256) -> Output (13x13x384)\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Max Pooling Layer: Input (13x13x384) -> Output (6x6x384)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Adaptive Average Pooling Layer: Input (6x6x384) -> Output (6x6x384)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Dropout Layer\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            # Fully Connected Layer #1: Input (6x6x384) -> Output (4096)\n",
    "            nn.Linear(384 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout Layer\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            # Fully Connected Layer #2: Input (4096) -> Output (4096)\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Fully Connected Layer #3: Input (4096) -> Output (num_classes)\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        # Flatten the features:\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796c4a9-85b8-4b75-9836-2a7208a36f57",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712101621403,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "4796c4a9-85b8-4b75-9836-2a7208a36f57"
   },
   "outputs": [],
   "source": [
    "def plot_training_curve(config, train_losses, val_losses, train_accs, val_accs, filename):\n",
    "    \"\"\"Plot training and validation losses and accuracies, then save the plot.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Training Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e58392-d3f9-4706-bb0f-0cccd8b0730f",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712101621403,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "b4e58392-d3f9-4706-bb0f-0cccd8b0730f"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, config):\n",
    "    \"\"\" Training loop. You should update this.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    training_start_time = time.process_time()\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) \n",
    "\n",
    "    # Keep track of the best validation loss and accuracy\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    # Lists to store the training and validation losses and accuracies\n",
    "    train_losses, valid_losses, train_accs, valid_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        start_of_epoch = time.process_time()\n",
    "        \n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss, total_train_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        train_accuracy = total_train_correct / (len(train_loader.dataset))\n",
    "        train_accs.append(train_accuracy)\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        total_valid_loss, total_valid_correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_valid_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "        valid_accuracy = total_valid_correct / (len(valid_loader.dataset))\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "        valid_accs.append(valid_accuracy)\n",
    "\n",
    "        # Update best validation loss and accuracy\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            best_valid_acc = valid_accuracy\n",
    "\n",
    "        time_elapsed = time.process_time() - start_of_epoch\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']} | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_valid_loss:.4f} | Train Acc: {train_accuracy:.4f} | Validation Acc: {valid_accuracy:.4f} | Time: {time_elapsed:.4f}\")\n",
    "\n",
    "    # Training curve plot\n",
    "    plot_filename = f\"training_curve_{model.__class__.__name__}_lr={config['learning_rate']}_epochs={config['num_epochs']}_batch={config['batch_size']}.png\"\n",
    "    plot_training_curve(config, train_losses, valid_losses, train_accs, valid_accs, plot_filename)\n",
    "\n",
    "    time_elapsed = time.process_time() - training_start_time\n",
    "    print(f\"Training complete in {time_elapsed:.2f}s\")\n",
    "    return best_valid_loss, best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e743c1e-8757-48e1-8372-6d1ea9945e1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 194909,
     "status": "error",
     "timestamp": 1712101816310,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "2e743c1e-8757-48e1-8372-6d1ea9945e1b",
    "outputId": "578312ae-441b-4d59-fb74-11280f56c78b"
   },
   "outputs": [],
   "source": [
    "hyperparameter_configs = [\n",
    "    # {'learning_rate': 1e-3, 'num_epochs': 50, 'batch_size': 128}, # Done\n",
    "    # {'learning_rate': 1e-3, 'num_epochs': 80, 'batch_size': 128}, # Done\n",
    "    {'learning_rate': 1e-3, 'num_epochs': 120, 'batch_size': 128}, # 120 epochs with OptimizedCNN128 gave best results. 200 epochs had overfitting.\n",
    "    # {'learning_rate': 1e-3, 'num_epochs': 200, 'batch_size': 128},\n",
    "    # {'learning_rate': 1e-4, 'num_epochs': 60, 'batch_size': 128},\n",
    "    # {'learning_rate': 1e-4, 'num_epochs': 120, 'batch_size': 128},\n",
    "    # {'learning_rate': 1e-5, 'num_epochs': 60, 'batch_size': 128},\n",
    "    # {'learning_rate': 1e-5, 'num_epochs': 120, 'batch_size': 128},\n",
    "    # {'learning_rate': 1e-5, 'num_epochs': 200, 'batch_size': 128},\n",
    "]\n",
    "\n",
    "performance_tracking = []\n",
    "# device = 'cpu'\n",
    "\n",
    "for config in hyperparameter_configs:\n",
    "    start = time.process_time()\n",
    "    \n",
    "    model = ModifiedAlexNet().to(device)\n",
    "    \n",
    "    print(f\"Training {model.__class__.__name__} model with config: {config}\")\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_trash_data_loader(batch_size=config[\"batch_size\"], img_dim=227)\n",
    "    \n",
    "    best_valid_loss, best_valid_acc = train(model, train_loader, val_loader, config)\n",
    "\n",
    "    # Track the performance for each config\n",
    "    performance_tracking.append({\n",
    "        'config': config,\n",
    "        'best_valid_acc': best_valid_acc,\n",
    "        'best_valid_loss': best_valid_loss\n",
    "    })\n",
    "\n",
    "    # Save the model after training\n",
    "    model_save_path = f\"{path_to_project}model_{model.__class__.__name__}_lr={config['learning_rate']}_epochs={config['num_epochs']}_batch={config['batch_size']}.pth\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    print(f\"Time elapsed: {time.process_time() - start}\")\n",
    "\n",
    "# Determine best performing model\n",
    "best_performance = sorted(performance_tracking, key=lambda x: x['best_valid_loss'])[0]\n",
    "print(\"Best performing model configuration:\", best_performance['config'])\n",
    "print(\"With Validation Loss:\", best_performance['best_valid_loss'], \"and Validation Accuracy:\", best_performance['best_valid_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176a8b7-8f1e-4914-b065-d5f5fa1487ba",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1712101816311,
     "user": {
      "displayName": "Hasan Iqbal",
      "userId": "05296353389571415320"
     },
     "user_tz": 240
    },
    "id": "1176a8b7-8f1e-4914-b065-d5f5fa1487ba"
   },
   "outputs": [],
   "source": [
    "def load_model(model_class, model_path):\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91734840-1a76-40a1-83d5-21f92b5fc7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, device, img_dim=128):\n",
    "    \"\"\"Predict the class for a single image.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_dim, img_dim)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "model_path = f\"{path_to_project}model_OptimizedLargeNet256_lr=0.001_epochs=60_batch=128.pth\"\n",
    "image_path = f\"{path_to_project}split_data/test/Paper/Paper_8.jpg\"\n",
    "\n",
    "model = load_model(OptimizedCNN128, model_path)\n",
    "\n",
    "predicted_class = predict_image(image_path, model, device)\n",
    "\n",
    "# 0 = Glass, 1 = Metal, 2 = Paper, 3 = Plastic\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
